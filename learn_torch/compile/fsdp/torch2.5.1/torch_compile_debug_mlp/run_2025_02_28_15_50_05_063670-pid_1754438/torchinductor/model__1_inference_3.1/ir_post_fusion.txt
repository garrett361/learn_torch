op1: ExternKernelSchedulerNode(FallbackKernel)
op1.writes = [StarDep(name='buf1', mode=None)]
op1.unmet_dependencies = []
op1.met_dependencies = [StarDep(name='arg8_1', mode=None)]
op1.outputs = [
    buf1: FallbackKernel
    buf1.layout = MultiOutputLayout(device=device(type='cuda', index=0))
    buf1.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op2'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op3'), can_inplace=False, is_weak=False),
    ]
]
op1.node.kernel = torch.ops.fsdp.all_gather_copy_in.default


op2: ExternKernelSchedulerNode(MultiOutput)
op2.writes = [StarDep(name='buf2', mode=None)]
op2.unmet_dependencies = [StarDep(name='buf1', mode=None)]
op2.met_dependencies = []
op2.outputs = [
    buf2: MultiOutput
    buf2.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf2.users = [NodeUser(node=ExternKernelSchedulerNode(name='op4'), can_inplace=False, is_weak=False)]
]
op2.node.kernel = None


op3: ExternKernelSchedulerNode(MultiOutput)
op3.writes = [StarDep(name='buf3', mode=None)]
op3.unmet_dependencies = [StarDep(name='buf1', mode=None)]
op3.met_dependencies = []
op3.outputs = [
    buf3: MultiOutput
    buf3.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf3.users = [NodeUser(node=ExternKernelSchedulerNode(name='op4'), can_inplace=False, is_weak=False)]
]
op3.node.kernel = None


op4: ExternKernelSchedulerNode(_CollectiveKernel)
op4.writes = 
    [   StarDep(name='buf4', mode=None),
        StarDep(name='buf5', mode=None),
        StarDep(name='buf6', mode=None)]
op4.unmet_dependencies = [StarDep(name='buf2', mode=None), StarDep(name='buf3', mode=None)]
op4.met_dependencies = []
op4.outputs = [
    buf4: _CollectiveKernel
    buf4.layout = <torch._inductor.ir.NoneLayout object at 0x150e13a4b740>
    buf4.aliases = ['buf2', 'buf3']
    buf4.users = [NodeUser(node=ExternKernelSchedulerNode(name='op4'), can_inplace=False, is_weak=False)]
    buf5: MutationOutput
    buf5.layout = <torch._inductor.ir.NoneLayout object at 0x150e1117d370>
    buf5.mutations = ['buf2']
    buf5.users = []
    buf6: MutationOutput
    buf6.layout = <torch._inductor.ir.NoneLayout object at 0x150e1117e0f0>
    buf6.mutations = ['buf3']
    buf6.users = [NodeUser(node=ExternKernelSchedulerNode(name='op5'), can_inplace=False, is_weak=False)]
]
op4.node.kernel = torch.ops._c10d_functional.all_gather_into_tensor_out.default


op0: NopKernelSchedulerNode(ComputedBuffer)
op0.writes = [MemoryDep('buf0', d0, {d0: 0}, None)]
op0.unmet_dependencies = []
op0.met_dependencies = []
op0.outputs = [
    buf0: ComputedBuffer
    buf0.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf0.users = [
        NodeUser(node=SchedulerNode(name='op6'), can_inplace=True, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op47'), can_inplace=False, is_weak=False),
    ]
]


op6: SchedulerNode(ComputedBuffer)
op6.writes = [MemoryDep('buf9', c0, {c0: 256}, None)]
op6.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 256}, None)]
op6.met_dependencies = []
op6.outputs = [
    buf9: ComputedBuffer
    buf9.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf9.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op7'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op8'), can_inplace=False, is_weak=False),
    ]
]
op6.group.device = cuda:0
op6.group.iteration = (256, 1)
op6.sizes = ([256], [])
buf0_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
buf9_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
class op6_loop_body:
    var_ranges = {z0: 256}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf9', get_index_1, load, None)
        return store
op6 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[256], 
        filename=__file__,
        triton_meta={'signature': {0: '*bf16', 1: '*bf16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 256
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
        tl.store(out_ptr0 + (x0), tmp0, xmask)


op9: SchedulerNode(ComputedBuffer)
op9.writes = [MemoryDep('buf15', c0, {c0: 16}, None)]
op9.unmet_dependencies = []
op9.met_dependencies = [MemoryDep('arg0_1', 0, {}, None), MemoryDep('arg6_1', c0, {c0: 16}, None)]
op9.outputs = [
    buf15: ComputedBuffer
    buf15.layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
    buf15.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op10'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op11'), can_inplace=False, is_weak=False),
    ]
]
op9.group.device = cuda:0
op9.group.iteration = (16, 1)
op9.sizes = ([16], [])
arg6_1_layout = FixedLayout('cuda', torch.bool, size=[1, 16], stride=[16, 1])
arg0_1_layout = FixedLayout('cuda', torch.bfloat16, size=[], stride=[])
buf15_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
class op9_loop_body:
    var_ranges = {z0: 16}
    index0 = z0
    index1 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg6_1', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg0_1', get_index_1)
        constant = ops.constant(0.0, torch.bfloat16)
        where = ops.where(load, constant, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf15', get_index_2, where, None)
        return store
op9 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[16], 
        filename=__file__,
        triton_meta={'signature': {0: '*i1', 1: '*bf16', 2: '*bf16', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 16
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (0)).to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK])
        tmp3 = 0.0
        tmp4 = tl.where(tmp0, tmp3, tmp2)
        tl.store(out_ptr0 + (x0), tmp4, xmask)


op12: NopKernelSchedulerNode(ComputedBuffer)
op12.writes = [MemoryDep('buf18', d0, {d0: 0}, None)]
op12.unmet_dependencies = []
op12.met_dependencies = []
op12.outputs = [
    buf18: ComputedBuffer
    buf18.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf18.users = [
        NodeUser(node=SchedulerNode(name='op18'), can_inplace=True, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op41'), can_inplace=False, is_weak=False),
    ]
]


op18: SchedulerNode(ComputedBuffer)
op18.writes = [MemoryDep('buf27', c0, {c0: 256}, None)]
op18.unmet_dependencies = [MemoryDep('buf18', c0, {c0: 256}, None)]
op18.met_dependencies = []
op18.outputs = [
    buf27: ComputedBuffer
    buf27.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf27.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op19'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op20'), can_inplace=False, is_weak=False),
    ]
]
op18.group.device = cuda:0
op18.group.iteration = (256, 1)
op18.sizes = ([256], [])
buf18_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
buf27_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
class op18_loop_body:
    var_ranges = {z0: 256}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf27', get_index_1, load, None)
        return store
op18 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[256], 
        filename=__file__,
        triton_meta={'signature': {0: '*bf16', 1: '*bf16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 256
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
        tl.store(out_ptr0 + (x0), tmp0, xmask)


op36: NopKernelSchedulerNode(ComputedBuffer)
op36.writes = [MemoryDep('buf54', d0, {d0: 0}, None)]
op36.unmet_dependencies = []
op36.met_dependencies = []
op36.outputs = [
    buf54: ComputedBuffer
    buf54.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf54.users = [NodeUser(node=ExternKernelSchedulerNode(name='op37'), can_inplace=False, is_weak=False)]
]


op5: ExternKernelSchedulerNode(_WaitKernel)
op5.writes = [StarDep(name='buf7', mode=None), StarDep(name='buf8', mode=None)]
op5.unmet_dependencies = [StarDep(name='buf6', mode=None)]
op5.met_dependencies = []
op5.outputs = [
    buf7: _WaitKernel
    buf7.layout = <torch._inductor.ir.NoneLayout object at 0x150e10fbcfe0>
    buf7.users = []
    buf8: MutationOutput
    buf8.layout = <torch._inductor.ir.NoneLayout object at 0x150e13d1ca70>
    buf8.mutations = ['buf3']
    buf8.users = [NodeUser(node=ExternKernelSchedulerNode(name='op7'), can_inplace=False, is_weak=False)]
]
op5.node.kernel = torch.ops._c10d_functional.wait_tensor.default


op7: ExternKernelSchedulerNode(FallbackKernel)
op7.writes = [StarDep(name='buf10', mode=None), StarDep(name='buf11', mode=None)]
op7.unmet_dependencies = [StarDep(name='buf8', mode=None), StarDep(name='buf9', mode=None)]
op7.met_dependencies = []
op7.outputs = [
    buf10: FallbackKernel
    buf10.layout = <torch._inductor.ir.NoneLayout object at 0x150e110fa0f0>
    buf10.aliases = ['buf9']
    buf10.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op7'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op8'), can_inplace=False, is_weak=False),
    ]
    buf11: MutationOutput
    buf11.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f5dfd0>
    buf11.mutations = ['buf9']
    buf11.users = [NodeUser(node=ExternKernelSchedulerNode(name='op8'), can_inplace=False, is_weak=False)]
]
op7.node.kernel = torch.ops.fsdp.split_with_sizes_copy.default


op8: ExternKernelSchedulerNode(SetSourceTensorKernel)
op8.writes = 
    [   StarDep(name='buf12', mode=None),
        StarDep(name='buf13', mode=None),
        StarDep(name='buf14', mode=None)]
op8.unmet_dependencies = [StarDep(name='buf11', mode=None)]
op8.met_dependencies = [StarDep(name='arg3_1', mode=None)]
op8.outputs = [
    buf12: SetSourceTensorKernel
    buf12.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf12.aliases = ['arg3_1', 'buf9']
    buf12.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op7'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op8'), can_inplace=False, is_weak=False),
    ]
    buf13: MutationOutput
    buf13.layout = <torch._inductor.ir.NoneLayout object at 0x150e110fb740>
    buf13.mutations = ['arg3_1']
    buf13.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op10'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
    buf14: MutationOutput
    buf14.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff09b0>
    buf14.mutations = ['buf9']
    buf14.users = [NodeUser(node=ExternKernelSchedulerNode(name='op47'), can_inplace=False, is_weak=True)]
]
op8.node.kernel = torch.ops.aten.set_.source_Tensor


op13: ExternKernelSchedulerNode(FallbackKernel)
op13.writes = [StarDep(name='buf19', mode=None)]
op13.unmet_dependencies = []
op13.met_dependencies = [StarDep(name='arg9_1', mode=None)]
op13.outputs = [
    buf19: FallbackKernel
    buf19.layout = MultiOutputLayout(device=device(type='cuda', index=0))
    buf19.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op14'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op15'), can_inplace=False, is_weak=False),
    ]
]
op13.node.kernel = torch.ops.fsdp.all_gather_copy_in.default


op14: ExternKernelSchedulerNode(MultiOutput)
op14.writes = [StarDep(name='buf20', mode=None)]
op14.unmet_dependencies = [StarDep(name='buf19', mode=None)]
op14.met_dependencies = []
op14.outputs = [
    buf20: MultiOutput
    buf20.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf20.users = [NodeUser(node=ExternKernelSchedulerNode(name='op16'), can_inplace=False, is_weak=False)]
]
op14.node.kernel = None


op15: ExternKernelSchedulerNode(MultiOutput)
op15.writes = [StarDep(name='buf21', mode=None)]
op15.unmet_dependencies = [StarDep(name='buf19', mode=None)]
op15.met_dependencies = []
op15.outputs = [
    buf21: MultiOutput
    buf21.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf21.users = [NodeUser(node=ExternKernelSchedulerNode(name='op16'), can_inplace=False, is_weak=False)]
]
op15.node.kernel = None


op16: ExternKernelSchedulerNode(_CollectiveKernel)
op16.writes = 
    [   StarDep(name='buf22', mode=None),
        StarDep(name='buf23', mode=None),
        StarDep(name='buf24', mode=None)]
op16.unmet_dependencies = [StarDep(name='buf20', mode=None), StarDep(name='buf21', mode=None)]
op16.met_dependencies = []
op16.outputs = [
    buf22: _CollectiveKernel
    buf22.layout = <torch._inductor.ir.NoneLayout object at 0x150e13d16630>
    buf22.aliases = ['buf20', 'buf21']
    buf22.users = [NodeUser(node=ExternKernelSchedulerNode(name='op16'), can_inplace=False, is_weak=False)]
    buf23: MutationOutput
    buf23.layout = <torch._inductor.ir.NoneLayout object at 0x150e13de8920>
    buf23.mutations = ['buf20']
    buf23.users = []
    buf24: MutationOutput
    buf24.layout = <torch._inductor.ir.NoneLayout object at 0x150e13cead20>
    buf24.mutations = ['buf21']
    buf24.users = [NodeUser(node=ExternKernelSchedulerNode(name='op17'), can_inplace=False, is_weak=False)]
]
op16.node.kernel = torch.ops._c10d_functional.all_gather_into_tensor_out.default


op10: ExternKernelSchedulerNode(ExternKernelOut)
op10.writes = [StarDep(name='buf16', mode=None)]
op10.unmet_dependencies = [StarDep(name='buf13', mode=None), StarDep(name='buf15', mode=None)]
op10.met_dependencies = []
op10.outputs = [
    buf16: ExternKernelOut
    buf16.layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
    buf16.users = [NodeUser(node=SchedulerNode(name='op21'), can_inplace=True, is_weak=False)]
]
op10.node.kernel = extern_kernels.mm


op21: SchedulerNode(ComputedBuffer)
op21.writes = [MemoryDep('buf33', c0, {c0: 16}, None)]
op21.unmet_dependencies = [MemoryDep('buf16', c0, {c0: 16}, None)]
op21.met_dependencies = [MemoryDep('arg5_1', c0, {c0: 16}, None)]
op21.outputs = [
    buf33: ComputedBuffer
    buf33.layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
    buf33.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op22'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op23'), can_inplace=False, is_weak=False),
    ]
]
op21.group.device = cuda:0
op21.group.iteration = (16, 1)
op21.sizes = ([16], [])
arg5_1_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
buf16_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
buf33_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
class op21_loop_body:
    var_ranges = {z0: 16}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg5_1', get_index)
        constant = ops.constant(0.0, torch.bfloat16)
        le = ops.le(load, constant)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('buf16', get_index_1)
        constant_1 = ops.constant(0.0, torch.bfloat16)
        where = ops.where(le, constant_1, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf33', get_index_2, where, None)
        return store
op21 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[16], 
        filename=__file__,
        triton_meta={'signature': {0: '*bf16', 1: '*bf16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 16
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
        tmp3 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tl.store(in_out_ptr0 + (x0), tmp4, xmask)


op17: ExternKernelSchedulerNode(_WaitKernel)
op17.writes = [StarDep(name='buf25', mode=None), StarDep(name='buf26', mode=None)]
op17.unmet_dependencies = [StarDep(name='buf24', mode=None)]
op17.met_dependencies = []
op17.outputs = [
    buf25: _WaitKernel
    buf25.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f5e840>
    buf25.users = []
    buf26: MutationOutput
    buf26.layout = <torch._inductor.ir.NoneLayout object at 0x150e13d16b70>
    buf26.mutations = ['buf21']
    buf26.users = [NodeUser(node=ExternKernelSchedulerNode(name='op19'), can_inplace=False, is_weak=False)]
]
op17.node.kernel = torch.ops._c10d_functional.wait_tensor.default


op19: ExternKernelSchedulerNode(FallbackKernel)
op19.writes = [StarDep(name='buf28', mode=None), StarDep(name='buf29', mode=None)]
op19.unmet_dependencies = [StarDep(name='buf26', mode=None), StarDep(name='buf27', mode=None)]
op19.met_dependencies = []
op19.outputs = [
    buf28: FallbackKernel
    buf28.layout = <torch._inductor.ir.NoneLayout object at 0x150e1393ddc0>
    buf28.aliases = ['buf27']
    buf28.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op19'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op20'), can_inplace=False, is_weak=False),
    ]
    buf29: MutationOutput
    buf29.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f725a0>
    buf29.mutations = ['buf27']
    buf29.users = [NodeUser(node=ExternKernelSchedulerNode(name='op20'), can_inplace=False, is_weak=False)]
]
op19.node.kernel = torch.ops.fsdp.split_with_sizes_copy.default


op20: ExternKernelSchedulerNode(SetSourceTensorKernel)
op20.writes = 
    [   StarDep(name='buf30', mode=None),
        StarDep(name='buf31', mode=None),
        StarDep(name='buf32', mode=None)]
op20.unmet_dependencies = [StarDep(name='buf29', mode=None)]
op20.met_dependencies = [StarDep(name='arg2_1', mode=None)]
op20.outputs = [
    buf30: SetSourceTensorKernel
    buf30.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf30.aliases = ['arg2_1', 'buf27']
    buf30.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op19'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op20'), can_inplace=False, is_weak=False),
    ]
    buf31: MutationOutput
    buf31.layout = <torch._inductor.ir.NoneLayout object at 0x150e13d17e60>
    buf31.mutations = ['arg2_1']
    buf31.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op22'), can_inplace=False, is_weak=False),
        NodeUser(node=OUTPUT, can_inplace=False, is_weak=False),
    ]
    buf32: MutationOutput
    buf32.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f72ba0>
    buf32.mutations = ['buf27']
    buf32.users = [NodeUser(node=ExternKernelSchedulerNode(name='op41'), can_inplace=False, is_weak=True)]
]
op20.node.kernel = torch.ops.aten.set_.source_Tensor


op25: ExternKernelSchedulerNode(FallbackKernel)
op25.writes = [StarDep(name='buf37', mode=None)]
op25.unmet_dependencies = []
op25.met_dependencies = [StarDep(name='arg10_1', mode=None)]
op25.outputs = [
    buf37: FallbackKernel
    buf37.layout = MultiOutputLayout(device=device(type='cuda', index=0))
    buf37.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op26'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op27'), can_inplace=False, is_weak=False),
    ]
]
op25.node.kernel = torch.ops.fsdp.all_gather_copy_in.default


op26: ExternKernelSchedulerNode(MultiOutput)
op26.writes = [StarDep(name='buf38', mode=None)]
op26.unmet_dependencies = [StarDep(name='buf37', mode=None)]
op26.met_dependencies = []
op26.outputs = [
    buf38: MultiOutput
    buf38.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf38.users = [NodeUser(node=ExternKernelSchedulerNode(name='op28'), can_inplace=False, is_weak=False)]
]
op26.node.kernel = None


op27: ExternKernelSchedulerNode(MultiOutput)
op27.writes = [StarDep(name='buf39', mode=None)]
op27.unmet_dependencies = [StarDep(name='buf37', mode=None)]
op27.met_dependencies = []
op27.outputs = [
    buf39: MultiOutput
    buf39.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf39.users = [NodeUser(node=ExternKernelSchedulerNode(name='op28'), can_inplace=False, is_weak=False)]
]
op27.node.kernel = None


op28: ExternKernelSchedulerNode(_CollectiveKernel)
op28.writes = 
    [   StarDep(name='buf40', mode=None),
        StarDep(name='buf41', mode=None),
        StarDep(name='buf42', mode=None)]
op28.unmet_dependencies = [StarDep(name='buf38', mode=None), StarDep(name='buf39', mode=None)]
op28.met_dependencies = []
op28.outputs = [
    buf40: _CollectiveKernel
    buf40.layout = <torch._inductor.ir.NoneLayout object at 0x150e110d8e30>
    buf40.aliases = ['buf38', 'buf39']
    buf40.users = [NodeUser(node=ExternKernelSchedulerNode(name='op28'), can_inplace=False, is_weak=False)]
    buf41: MutationOutput
    buf41.layout = <torch._inductor.ir.NoneLayout object at 0x150e110fb590>
    buf41.mutations = ['buf38']
    buf41.users = []
    buf42: MutationOutput
    buf42.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff0470>
    buf42.mutations = ['buf39']
    buf42.users = [NodeUser(node=ExternKernelSchedulerNode(name='op29'), can_inplace=False, is_weak=False)]
]
op28.node.kernel = torch.ops._c10d_functional.all_gather_into_tensor_out.default


op22: ExternKernelSchedulerNode(ExternKernelOut)
op22.writes = [StarDep(name='buf34', mode=None)]
op22.unmet_dependencies = [StarDep(name='buf31', mode=None), StarDep(name='buf33', mode=None)]
op22.met_dependencies = []
op22.outputs = [
    buf34: ExternKernelOut
    buf34.layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
    buf34.users = [NodeUser(node=SchedulerNode(name='op33'), can_inplace=True, is_weak=False)]
]
op22.node.kernel = extern_kernels.mm


op33: SchedulerNode(ComputedBuffer)
op33.writes = [MemoryDep('buf51', c0, {c0: 16}, None)]
op33.unmet_dependencies = [MemoryDep('buf34', c0, {c0: 16}, None)]
op33.met_dependencies = [MemoryDep('arg4_1', c0, {c0: 16}, None)]
op33.outputs = [
    buf51: ComputedBuffer
    buf51.layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
    buf51.users = [NodeUser(node=ExternKernelSchedulerNode(name='op34'), can_inplace=False, is_weak=False)]
]
op33.group.device = cuda:0
op33.group.iteration = (16, 1)
op33.sizes = ([16], [])
arg4_1_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
buf34_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
buf51_layout = FixedLayout('cuda', torch.bfloat16, size=[1, 16], stride=[16, 1])
class op33_loop_body:
    var_ranges = {z0: 16}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg4_1', get_index)
        constant = ops.constant(0.0, torch.bfloat16)
        le = ops.le(load, constant)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('buf34', get_index_1)
        constant_1 = ops.constant(0.0, torch.bfloat16)
        where = ops.where(le, constant_1, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf51', get_index_2, where, None)
        return store
op33 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[16], 
        filename=__file__,
        triton_meta={'signature': {0: '*bf16', 1: '*bf16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 16
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
        tmp3 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tl.store(in_out_ptr0 + (x0), tmp4, xmask)


op34: ExternKernelSchedulerNode(ExternKernelOut)
op34.writes = [StarDep(name='buf52', mode=None)]
op34.unmet_dependencies = [StarDep(name='buf51', mode=None)]
op34.met_dependencies = [StarDep(name='arg1_1', mode=None)]
op34.outputs = [
    buf52: ExternKernelOut
    buf52.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf52.users = [NodeUser(node=ExternKernelSchedulerNode(name='op37'), can_inplace=False, is_weak=False)]
]
op34.node.kernel = extern_kernels.mm


op37: ExternKernelSchedulerNode(FallbackKernel)
op37.writes = [StarDep(name='buf55', mode=None), StarDep(name='buf56', mode=None)]
op37.unmet_dependencies = [StarDep(name='buf52', mode=None), StarDep(name='buf54', mode=None)]
op37.met_dependencies = []
op37.outputs = [
    buf55: FallbackKernel
    buf55.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f727e0>
    buf55.aliases = ['buf54']
    buf55.users = [NodeUser(node=ExternKernelSchedulerNode(name='op37'), can_inplace=False, is_weak=False)]
    buf56: MutationOutput
    buf56.layout = <torch._inductor.ir.NoneLayout object at 0x150e10fbcfb0>
    buf56.mutations = ['buf54']
    buf56.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op39'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op40'), can_inplace=False, is_weak=False),
    ]
]
op37.node.kernel = torch.ops.fsdp.chunk_cat.default


op39: ExternKernelSchedulerNode(_CollectiveKernel)
op39.writes = [StarDep(name='buf58', mode=None)]
op39.unmet_dependencies = [StarDep(name='buf56', mode=None)]
op39.met_dependencies = []
op39.outputs = [
    buf58: _CollectiveKernel
    buf58.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf58.users = [NodeUser(node=ExternKernelSchedulerNode(name='op40'), can_inplace=False, is_weak=False)]
]
op39.node.kernel = torch.ops._c10d_functional.reduce_scatter_tensor.default


op42: NopKernelSchedulerNode(ComputedBuffer)
op42.writes = [MemoryDep('buf62', d0, {d0: 0}, None)]
op42.unmet_dependencies = []
op42.met_dependencies = []
op42.outputs = [
    buf62: ComputedBuffer
    buf62.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf62.users = [NodeUser(node=ExternKernelSchedulerNode(name='op43'), can_inplace=False, is_weak=False)]
]


op23: ExternKernelSchedulerNode(ExternKernelOut)
op23.writes = [StarDep(name='buf35', mode=None)]
op23.unmet_dependencies = [StarDep(name='buf33', mode=None)]
op23.met_dependencies = [StarDep(name='arg4_1', mode=None)]
op23.outputs = [
    buf35: ExternKernelOut
    buf35.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf35.users = [NodeUser(node=ExternKernelSchedulerNode(name='op43'), can_inplace=False, is_weak=False)]
]
op23.node.kernel = extern_kernels.mm


op11: ExternKernelSchedulerNode(ExternKernelOut)
op11.writes = [StarDep(name='buf17', mode=None)]
op11.unmet_dependencies = [StarDep(name='buf15', mode=None)]
op11.met_dependencies = [StarDep(name='arg5_1', mode=None)]
op11.outputs = [
    buf17: ExternKernelOut
    buf17.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf17.users = [NodeUser(node=ExternKernelSchedulerNode(name='op49'), can_inplace=False, is_weak=False)]
]
op11.node.kernel = extern_kernels.mm


op48: NopKernelSchedulerNode(ComputedBuffer)
op48.writes = [MemoryDep('buf70', d0, {d0: 0}, None)]
op48.unmet_dependencies = []
op48.met_dependencies = []
op48.outputs = [
    buf70: ComputedBuffer
    buf70.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf70.users = [NodeUser(node=ExternKernelSchedulerNode(name='op49'), can_inplace=False, is_weak=False)]
]


op24: NopKernelSchedulerNode(ComputedBuffer)
op24.writes = [MemoryDep('buf36', d0, {d0: 0}, None)]
op24.unmet_dependencies = []
op24.met_dependencies = []
op24.outputs = [
    buf36: ComputedBuffer
    buf36.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf36.users = [
        NodeUser(node=SchedulerNode(name='op30'), can_inplace=True, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op35'), can_inplace=False, is_weak=False),
    ]
]


op30: SchedulerNode(ComputedBuffer)
op30.writes = [MemoryDep('buf45', c0, {c0: 256}, None)]
op30.unmet_dependencies = [MemoryDep('buf36', c0, {c0: 256}, None)]
op30.met_dependencies = []
op30.outputs = [
    buf45: ComputedBuffer
    buf45.layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
    buf45.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op31'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op32'), can_inplace=False, is_weak=False),
    ]
]
op30.group.device = cuda:0
op30.group.iteration = (256, 1)
op30.sizes = ([256], [])
buf36_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
buf45_layout = FixedLayout('cuda', torch.bfloat16, size=[256], stride=[1])
class op30_loop_body:
    var_ranges = {z0: 256}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf36', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf45', get_index_1, load, None)
        return store
op30 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[256], 
        filename=__file__,
        triton_meta={'signature': {0: '*bf16', 1: '*bf16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 256
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
        tl.store(out_ptr0 + (x0), tmp0, xmask)


op47: ExternKernelSchedulerNode(ResizeStorageBytes)
op47.writes = [StarDep(name='buf69', mode=None)]
op47.unmet_dependencies = [StarDep(name='buf0', mode=None), WeakDep(name='buf14', mutating_buf='buf69')]
op47.met_dependencies = []
op47.outputs = [
    buf69: ResizeStorageBytes
    buf69.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff72c0>
    buf69.mutations = ['buf0']
    buf69.users = []
]
op47.node.kernel = inductor_ops.resize_storage_bytes_


op41: ExternKernelSchedulerNode(ResizeStorageBytes)
op41.writes = [StarDep(name='buf61', mode=None)]
op41.unmet_dependencies = [StarDep(name='buf18', mode=None), WeakDep(name='buf32', mutating_buf='buf61')]
op41.met_dependencies = []
op41.outputs = [
    buf61: ResizeStorageBytes
    buf61.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff57c0>
    buf61.mutations = ['buf18']
    buf61.users = []
]
op41.node.kernel = inductor_ops.resize_storage_bytes_


op29: ExternKernelSchedulerNode(_WaitKernel)
op29.writes = [StarDep(name='buf43', mode=None), StarDep(name='buf44', mode=None)]
op29.unmet_dependencies = [StarDep(name='buf42', mode=None)]
op29.met_dependencies = []
op29.outputs = [
    buf43: _WaitKernel
    buf43.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff05c0>
    buf43.users = []
    buf44: MutationOutput
    buf44.layout = <torch._inductor.ir.NoneLayout object at 0x150e110d8a70>
    buf44.mutations = ['buf39']
    buf44.users = [NodeUser(node=ExternKernelSchedulerNode(name='op31'), can_inplace=False, is_weak=False)]
]
op29.node.kernel = torch.ops._c10d_functional.wait_tensor.default


op31: ExternKernelSchedulerNode(FallbackKernel)
op31.writes = [StarDep(name='buf46', mode=None), StarDep(name='buf47', mode=None)]
op31.unmet_dependencies = [StarDep(name='buf44', mode=None), StarDep(name='buf45', mode=None)]
op31.met_dependencies = []
op31.outputs = [
    buf46: FallbackKernel
    buf46.layout = <torch._inductor.ir.NoneLayout object at 0x150e110f8350>
    buf46.aliases = ['buf45']
    buf46.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op31'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op32'), can_inplace=False, is_weak=False),
    ]
    buf47: MutationOutput
    buf47.layout = <torch._inductor.ir.NoneLayout object at 0x150e10e186b0>
    buf47.mutations = ['buf45']
    buf47.users = [NodeUser(node=ExternKernelSchedulerNode(name='op32'), can_inplace=False, is_weak=False)]
]
op31.node.kernel = torch.ops.fsdp.split_with_sizes_copy.default


op32: ExternKernelSchedulerNode(SetSourceTensorKernel)
op32.writes = 
    [   StarDep(name='buf48', mode=None),
        StarDep(name='buf49', mode=None),
        StarDep(name='buf50', mode=None)]
op32.unmet_dependencies = [StarDep(name='buf47', mode=None)]
op32.met_dependencies = [StarDep(name='arg7_1', mode=None)]
op32.outputs = [
    buf48: SetSourceTensorKernel
    buf48.layout = FixedLayout('cuda', torch.bfloat16, size=[16, 16], stride=[16, 1])
    buf48.aliases = ['arg7_1', 'buf45']
    buf48.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op31'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op32'), can_inplace=False, is_weak=False),
    ]
    buf49: MutationOutput
    buf49.layout = <torch._inductor.ir.NoneLayout object at 0x150e10e1bb90>
    buf49.mutations = ['arg7_1']
    buf49.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
    buf50: MutationOutput
    buf50.layout = <torch._inductor.ir.NoneLayout object at 0x150e10e1a3c0>
    buf50.mutations = ['buf45']
    buf50.users = [NodeUser(node=ExternKernelSchedulerNode(name='op35'), can_inplace=False, is_weak=True)]
]
op32.node.kernel = torch.ops.aten.set_.source_Tensor


op35: ExternKernelSchedulerNode(ResizeStorageBytes)
op35.writes = [StarDep(name='buf53', mode=None)]
op35.unmet_dependencies = [StarDep(name='buf36', mode=None), WeakDep(name='buf50', mutating_buf='buf53')]
op35.met_dependencies = []
op35.outputs = [
    buf53: ResizeStorageBytes
    buf53.layout = <torch._inductor.ir.NoneLayout object at 0x150e10e1bfb0>
    buf53.mutations = ['buf36']
    buf53.users = []
]
op35.node.kernel = inductor_ops.resize_storage_bytes_


op40: ExternKernelSchedulerNode(_WaitKernel)
op40.writes = [StarDep(name='buf59', mode=None), StarDep(name='buf60', mode=None)]
op40.unmet_dependencies = [StarDep(name='buf56', mode=None), StarDep(name='buf58', mode=None)]
op40.met_dependencies = []
op40.outputs = [
    buf59: _WaitKernel
    buf59.layout = <torch._inductor.ir.NoneLayout object at 0x150e13919910>
    buf59.users = []
    buf60: MutationOutput
    buf60.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff53a0>
    buf60.mutations = ['buf58']
    buf60.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op40.node.kernel = torch.ops._c10d_functional.wait_tensor.default


op43: ExternKernelSchedulerNode(FallbackKernel)
op43.writes = [StarDep(name='buf63', mode=None), StarDep(name='buf64', mode=None)]
op43.unmet_dependencies = [StarDep(name='buf35', mode=None), StarDep(name='buf62', mode=None)]
op43.met_dependencies = []
op43.outputs = [
    buf63: FallbackKernel
    buf63.layout = <torch._inductor.ir.NoneLayout object at 0x150e10f5e990>
    buf63.aliases = ['buf62']
    buf63.users = [NodeUser(node=ExternKernelSchedulerNode(name='op43'), can_inplace=False, is_weak=False)]
    buf64: MutationOutput
    buf64.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff6390>
    buf64.mutations = ['buf62']
    buf64.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op45'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op46'), can_inplace=False, is_weak=False),
    ]
]
op43.node.kernel = torch.ops.fsdp.chunk_cat.default


op45: ExternKernelSchedulerNode(_CollectiveKernel)
op45.writes = [StarDep(name='buf66', mode=None)]
op45.unmet_dependencies = [StarDep(name='buf64', mode=None)]
op45.met_dependencies = []
op45.outputs = [
    buf66: _CollectiveKernel
    buf66.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf66.users = [NodeUser(node=ExternKernelSchedulerNode(name='op46'), can_inplace=False, is_weak=False)]
]
op45.node.kernel = torch.ops._c10d_functional.reduce_scatter_tensor.default


op46: ExternKernelSchedulerNode(_WaitKernel)
op46.writes = [StarDep(name='buf67', mode=None), StarDep(name='buf68', mode=None)]
op46.unmet_dependencies = [StarDep(name='buf64', mode=None), StarDep(name='buf66', mode=None)]
op46.met_dependencies = []
op46.outputs = [
    buf67: _WaitKernel
    buf67.layout = <torch._inductor.ir.NoneLayout object at 0x150e110da1b0>
    buf67.users = []
    buf68: MutationOutput
    buf68.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff7260>
    buf68.mutations = ['buf66']
    buf68.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op46.node.kernel = torch.ops._c10d_functional.wait_tensor.default


op49: ExternKernelSchedulerNode(FallbackKernel)
op49.writes = [StarDep(name='buf71', mode=None), StarDep(name='buf72', mode=None)]
op49.unmet_dependencies = [StarDep(name='buf17', mode=None), StarDep(name='buf70', mode=None)]
op49.met_dependencies = []
op49.outputs = [
    buf71: FallbackKernel
    buf71.layout = <torch._inductor.ir.NoneLayout object at 0x150e110fa8a0>
    buf71.aliases = ['buf70']
    buf71.users = [NodeUser(node=ExternKernelSchedulerNode(name='op49'), can_inplace=False, is_weak=False)]
    buf72: MutationOutput
    buf72.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff76e0>
    buf72.mutations = ['buf70']
    buf72.users = [
        NodeUser(node=ExternKernelSchedulerNode(name='op51'), can_inplace=False, is_weak=False),
        NodeUser(node=ExternKernelSchedulerNode(name='op52'), can_inplace=False, is_weak=False),
    ]
]
op49.node.kernel = torch.ops.fsdp.chunk_cat.default


op51: ExternKernelSchedulerNode(_CollectiveKernel)
op51.writes = [StarDep(name='buf74', mode=None)]
op51.unmet_dependencies = [StarDep(name='buf72', mode=None)]
op51.met_dependencies = []
op51.outputs = [
    buf74: _CollectiveKernel
    buf74.layout = FixedLayout('cuda', torch.bfloat16, size=[128], stride=[1])
    buf74.users = [NodeUser(node=ExternKernelSchedulerNode(name='op52'), can_inplace=False, is_weak=False)]
]
op51.node.kernel = torch.ops._c10d_functional.reduce_scatter_tensor.default


op52: ExternKernelSchedulerNode(_WaitKernel)
op52.writes = [StarDep(name='buf75', mode=None), StarDep(name='buf76', mode=None)]
op52.unmet_dependencies = [StarDep(name='buf72', mode=None), StarDep(name='buf74', mode=None)]
op52.met_dependencies = []
op52.outputs = [
    buf75: _WaitKernel
    buf75.layout = <torch._inductor.ir.NoneLayout object at 0x150e110da1e0>
    buf75.users = []
    buf76: MutationOutput
    buf76.layout = <torch._inductor.ir.NoneLayout object at 0x150e10ff78c0>
    buf76.mutations = ['buf74']
    buf76.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op52.node.kernel = torch.ops._c10d_functional.wait_tensor.default


