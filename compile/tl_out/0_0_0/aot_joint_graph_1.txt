class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "f32[128, 128]"; primals_2: "f32[128, 128]"; primals_3: "f32[1, 128]"; tangents_1: "f32[1, 128]"; 
    
        primals_1, primals_2, primals_3, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
        # File: /gpfs/users/goon/github/garrett361/learn_torch/compile/basic.py:18 in forward, code: outputs = self.lin1(self.lin0(inputs).relu())
        permute: "f32[128, 128]" = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None
        mm: "f32[1, 128]" = torch.ops.aten.mm.default(primals_3, permute);  permute = None
        relu: "f32[1, 128]" = torch.ops.aten.relu.default(mm);  mm = None
        alias: "f32[1, 128]" = torch.ops.aten.alias.default(relu)
        alias_1: "f32[1, 128]" = torch.ops.aten.alias.default(alias);  alias = None
        permute_1: "f32[128, 128]" = torch.ops.aten.permute.default(primals_2, [1, 0]);  primals_2 = None
        mm_1: "f32[1, 128]" = torch.ops.aten.mm.default(relu, permute_1)
        permute_2: "f32[128, 1]" = torch.ops.aten.permute.default(tangents_1, [1, 0])
        mm_2: "f32[128, 128]" = torch.ops.aten.mm.default(permute_2, relu);  permute_2 = relu = None
        permute_3: "f32[128, 128]" = torch.ops.aten.permute.default(mm_2, [1, 0]);  mm_2 = None
        permute_4: "f32[128, 128]" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
        mm_3: "f32[1, 128]" = torch.ops.aten.mm.default(tangents_1, permute_4);  tangents_1 = permute_4 = None
        permute_5: "f32[128, 128]" = torch.ops.aten.permute.default(permute_3, [1, 0]);  permute_3 = None
        alias_2: "f32[1, 128]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 128]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        le: "b8[1, 128]" = torch.ops.aten.le.Scalar(alias_3, 0);  alias_3 = None
        scalar_tensor: "f32[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
        where: "f32[1, 128]" = torch.ops.aten.where.self(le, scalar_tensor, mm_3);  le = scalar_tensor = mm_3 = None
        permute_6: "f32[128, 1]" = torch.ops.aten.permute.default(where, [1, 0]);  where = None
        mm_4: "f32[128, 128]" = torch.ops.aten.mm.default(permute_6, primals_3);  permute_6 = primals_3 = None
        permute_7: "f32[128, 128]" = torch.ops.aten.permute.default(mm_4, [1, 0]);  mm_4 = None
        permute_8: "f32[128, 128]" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
        return pytree.tree_unflatten([mm_1, permute_8, permute_5, None], self._out_spec)
        