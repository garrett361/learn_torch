class GraphModule(torch.nn.Module):
    def forward(self, primals_3: "f32[1, 128]", relu: "f32[1, 128]", permute_4: "f32[128, 128]", tangents_1: "f32[1, 128]"):
        # File: /gpfs/users/goon/github/garrett361/learn_torch/compile/basic.py:18 in forward, code: outputs = self.lin1(self.lin0(inputs).relu())
        alias: "f32[1, 128]" = torch.ops.aten.alias.default(relu)
        alias_1: "f32[1, 128]" = torch.ops.aten.alias.default(alias);  alias = None
        permute_2: "f32[128, 1]" = torch.ops.aten.permute.default(tangents_1, [1, 0])
        mm_2: "f32[128, 128]" = torch.ops.aten.mm.default(permute_2, relu);  permute_2 = relu = None
        permute_3: "f32[128, 128]" = torch.ops.aten.permute.default(mm_2, [1, 0]);  mm_2 = None
        mm_3: "f32[1, 128]" = torch.ops.aten.mm.default(tangents_1, permute_4);  tangents_1 = permute_4 = None
        permute_5: "f32[128, 128]" = torch.ops.aten.permute.default(permute_3, [1, 0]);  permute_3 = None
        alias_2: "f32[1, 128]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 128]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        le: "b8[1, 128]" = torch.ops.aten.le.Scalar(alias_3, 0);  alias_3 = None
        full_default: "f32[]" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where: "f32[1, 128]" = torch.ops.aten.where.self(le, full_default, mm_3);  le = full_default = mm_3 = None
        permute_6: "f32[128, 1]" = torch.ops.aten.permute.default(where, [1, 0]);  where = None
        mm_4: "f32[128, 128]" = torch.ops.aten.mm.default(permute_6, primals_3);  permute_6 = primals_3 = None
        permute_7: "f32[128, 128]" = torch.ops.aten.permute.default(mm_4, [1, 0]);  mm_4 = None
        permute_8: "f32[128, 128]" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
        return [permute_8, permute_5, None]
        